import os
import json
import logging
from typing import Any, Dict, List, Optional, Tuple
import base64
import argparse
from openai import OpenAI
from pixrefer.core.utils import load_data, load_prompt, load_config

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class GPTAnnotator:
    """A class for annotating images with bounding boxes using GPT models."""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        """Initialize the GPT annotator.
        
        Args:
            config: Configuration dictionary containing API key and other settings.
        """
        self.config = config or {}
        self.client = self._setup_client()
        
    def _setup_client(self) -> Any:
        """Set up the OpenAI client.
        
        Returns:
            An OpenAI client instance.
        """
        api_key = self.config.get('api', {}).get('openai', {}).get('key')
        if not api_key:
            raise ValueError("OpenAI API key is required. Please provide it in the config file.")
        
        return OpenAI(api_key=api_key)
    
    def _encode_image(self, image_path: str) -> str:
        """Encode an image as a base64 string.
        
        Args:
            image_path: Path to the image.
            
        Returns:
            Base64 encoded image string.
        """
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    
    def annotate_single_item(self, 
                            item: Dict[str, Any],
                            image_path: str,
                            prompt: str = None,
                            model: str = 'gpt-4o') -> str:
        """Annotate a single item with a pre-boxed image.
        
        Args:
            item: Item containing image and mask information.
            image_path: Path to the image.
            prompt: A specific prompt to use.
            model: GPT model to use.
            
        Returns:
            Description generated by GPT.
        """
        
        # Encode the pre-boxed image
        base64_image = self._encode_image(image_path)

        try:
            # Call the GPT model
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                    ]}
                ]
            )
            
            # Return the GPT description
            return response.choices[0].message.content
        
        except Exception as e:
            logger.error(f"Error calling GPT API: {e}")
            return f"Error generating description: {str(e)}"
    
    def run(self, 
            json_path: str,
            boxed_image_dir: str,
            output_json_path: str,
            max_samples: int = 5, 
            concise: bool = False,
            model: str = 'gpt-4o') -> List[Dict[str, Any]]:
        """Run the annotation process.
        
        Args:
            json_path: Path to the JSON file containing a list of items.
            boxed_image_dir: Directory containing pre-boxed images.
            output_json_path: Path to output JSON file.
            max_samples: Maximum number of samples to process.
            concise: Whether to use concise prompt.
            model: GPT model to use.
            
        Returns:
            List of annotation results.
        """
        results = []
        
        # Read the JSON file containing a list of items
        try:
            samples = load_data(json_path)
                
            # Ensure samples is a list
            if not isinstance(samples, list):
                raise ValueError(f"JSON file {json_path} must contain a list of items")
                
            # Limit to max_samples
            samples = samples[:max_samples]
                
        except Exception as e:
            logger.error(f"Error loading JSON file {json_path}: {e}")
            return []
        
        # Process each sample
        for i, sample in enumerate(samples):
            try:
                image_id = sample["image_id"]
                logger.info(f"Processing item {i+1}/{len(samples)}: {image_id}")
                
                # Get the path to the pre-boxed image
                boxed_image_path = os.path.join(boxed_image_dir, sample['boxed_image_path'])

                # Prompt number is not None
                # if prompt_number is None:
                # rompt_number = str(random.randint(1, 10))
                prompt_number = sample['prompt_number']
                if concise is True:
                    prompt_name = f'box.concise.description{prompt_number}'
                else:
                    prompt_name = f'box.regular.description{prompt_number}'
                print(f'prompt_name: {prompt_name}')
                try:
                    prompt = load_prompt(prompt_name)
                except Exception as e:
                    logger.error(f'Failed to load prompt. Using default prompt. Error: {e}')

                # Annotate the item
                description = self.annotate_single_item(
                    item=sample,
                    image_path=boxed_image_path,
                    prompt=prompt,
                    model=model
                )
                # Prepare the result data
                result = sample
                result['gpt_description'] = description
                result['model_info'] = 'gpt-4o'
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing sample {i}: {e}")

        # Write all results to a single JSON file
        try:
            with open(output_json_path, 'w') as out_file:
                json.dump(results, out_file, indent=2)
            logger.info(f"Results saved to {output_json_path}")
        except Exception as e:
            logger.error(f"Error writing results to {output_json_path}: {e}")
    
        return results


def parse_args_and_get_config(
    description: str = "Annotate images with bounding boxes using GPT models",
    parser: Optional[argparse.ArgumentParser] = None
) -> Tuple[argparse.Namespace, Dict[str, Any], Dict[str, Any]]:
    """Parse command line arguments and load configuration.
    
    Args:
        description: Description of the program for the help message.
        parser: Optional existing ArgumentParser to add arguments to.
        
    Returns:
        Tuple containing:
            - The parsed arguments
            - The loaded configuration
            - A dictionary of processed parameters
    """
    
    # Create parser if not provided
    if parser is None:
        parser = argparse.ArgumentParser(description=description)
    
    # Add common arguments
    parser.add_argument("--concise", type=bool, default=False,
                        help="Use concise prompt")
    parser.add_argument("--json_path", type=str, 
                        help="Path to JSON file containing a list of items")
    parser.add_argument("--boxed_image_dir", type=str, 
                        help="Directory containing pre-boxed images")
    parser.add_argument("--output_json_path", type=str, 
                        help="Path to output JSON file")
    parser.add_argument("--max_samples", type=int, 
                        help="Maximum number of samples to process")
    parser.add_argument("--model", type=str, 
                        help="GPT model to use")
    
    args = parser.parse_args()
    config = load_config()
    
    # Process parameters
    parameters = {
        "json_path": args.json_path or config.get('data', {}).get('json_path'),
        "boxed_image_dir": args.boxed_image_dir or config.get('data', {}).get('boxed_image_dir'),
        "output_json_path": args.output_json_path or config.get('data', {}).get('output_json_path'),
        "max_samples": args.max_samples or config.get('max_samples',5),
        "concise": args.concise,
        "model": args.model or config.get('api', {}).get('openai', {}).get('model', 'gpt-4o')
    }
    
    # Log the parameters
    logger.info(f"Using parameters:")
    for key, value in parameters.items():
        logger.info(f"  {key}: {value}")
    
    return args, config, parameters


def main():
    """Main function to run the annotator."""
    args, config, parameters = parse_args_and_get_config()
    
    # Validate required parameters
    if not parameters["json_path"] or not parameters["boxed_image_dir"]:
        raise ValueError("JSON path and boxed images directory must be provided either in config or as command line arguments.")
    
    annotator = GPTAnnotator(config)
    results = annotator.run(
        json_path=parameters["json_path"],
        boxed_image_dir=parameters["boxed_image_dir"],
        output_json_path=parameters["output_json_path"],
        max_samples=parameters["max_samples"],
        concise=parameters["concise"],
        model=parameters["model"]
    )
    
    logger.info(f"Processing complete. Annotated {len(results)} images.")


if __name__ == "__main__":
    main() 